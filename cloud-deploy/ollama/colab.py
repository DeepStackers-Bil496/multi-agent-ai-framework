# Colabda bunlari yorum satirindan cikarak calistirin
#!pkill ollama
#!pkill ngrok
#!pip install pyngrok
#!curl -fsSL https://ollama.com/install.sh | sh

import os
import threading
import time
import subprocess
from pyngrok import ngrok, conf

# --- AYARLAR ---
NGROK_AUTH_TOKEN = "37kVEwa8EZtr5XhBqWf3FQAnkN2_4WhXosh1h74P7iocrdu8L" # TokenÄ±nÄ± buraya yapÄ±ÅŸtÄ±r

# Ä°stediÄŸin modelleri buraya LÄ°STE olarak yaz:
# Ã–neri: Bir tane kodlama, bir tane genel zeka, bir tane hÄ±zlÄ± model olsun.
MODELS_TO_PULL = [
    "gpt-oss:20b",
    "qwen2.5:14b"         # Google'Ä±n modeli
]
# ---------------

# 2. Ngrok yetkilendirmesi
conf.get_default().auth_token = NGROK_AUTH_TOKEN

# 3. Ortam DeÄŸiÅŸkenleri (CORS ve Host ayarÄ±)
my_env = os.environ.copy()
my_env["OLLAMA_ORIGINS"] = "*"
my_env["OLLAMA_HOST"] = "0.0.0.0"

# 4. Ollama server'Ä± baÅŸlat
def start_ollama_server():
    print("Ollama server baÅŸlatÄ±lÄ±yor...")
    subprocess.Popen(["ollama", "serve"], env=my_env, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

ollama_thread = threading.Thread(target=start_ollama_server)
ollama_thread.start()

time.sleep(10) # Server kendine gelsin

# 5. Modelleri DÃ¶ngÃ¼ ile Ä°ndir
print(f"Toplam {len(MODELS_TO_PULL)} model indirilecek. Bu biraz sÃ¼rebilir...")

for model in MODELS_TO_PULL:
    print(f"â¬‡ï¸ Ä°ndiriliyor: {model} ...")
    subprocess.run(["ollama", "pull", model], env=my_env)
    print(f"âœ… {model} hazÄ±r!")

# 6. TÃ¼neli aÃ§
try:
    public_url = ngrok.connect(11434).public_url
    print("\n" + "="*50)
    print(f"ğŸ”¥ TÃœM MODELLER HAZIR! API Adresi:")
    print(f"\nğŸ‘‰ {public_url} ğŸ‘ˆ\n")
    print(f"KullanÄ±labilir Modeller: {', '.join(MODELS_TO_PULL)}")
    print("="*50)

except Exception as e:
    print("Hata:", e)

# DÃ¶ngÃ¼
try:
    while True:
        time.sleep(1)
except KeyboardInterrupt:
    ngrok.kill()